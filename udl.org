* Notebook 1.1 --- Background Mathematics

#+begin_src jupyter-julia :session jl :async yes
using Plots, LaTeXStrings, Measures, Printf, LinearAlgebra

pgfplotsx()
PGFPlotsX.CUSTOM_PREAMBLE = [raw"\usepackage{newpx}"]
#+end_src

#+RESULTS:
: 1-element Vector{String}:
:  "\\usepackage{newpx}"

#+begin_src jupyter-julia :session jl :async yes
f(x; Î²=0, Ï‰=1) = Î² + Ï‰ * x

p = plot(x -> f(x, Î²=10, Ï‰=-2), 0:10,
    xlims=(0, 10),
    ylims=(0, 10),
    xlabel=L"x",
    ylabel=L"f(x)",
    title="1D linear function",
    legend=false,
    titlefontsize = 20,
    guidefontsize = 20,
    tickfontsize = 14,
    background_color=:transparent,
    background_color_inside=:transparent
)

io = IOBuffer()
show(io, MIME("image/svg+xml"), p)
svg_content = String(take!(io))
cleaned_svg = replace(svg_content, r"<path[^>]*fill=\"rgb\(100%, 100%, 100%\)\"[^>]*/>" => "")
display("image/svg+xml", cleaned_svg)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/c3a14894398e4b96f5121a06bf7a193fb9c726ee.svg]]

#+begin_src jupyter-julia :session jl :async yes
f(x1, x2; Î², Ï‰1, Ï‰2) = Î² + Ï‰1 * x1 + Ï‰2 * x2
fâ€²(x1, x2) = f(x1, x2, Î²=0, Ï‰1=1, Ï‰2=-0.5)

p_sf = surface(1:10, 1:10, fâ€²,
    zlabel=L"y",
    guidefontsize=14
)
p_ct = contour(1:10, 1:10, fâ€²,
    guidefontsize=18
)

plot(p_sf, p_ct,
    legend=false,
    title="2D linear function",
    xlabel=L"x_1",
    ylabel=L"x_2",
    size=(1000, 500),
    margin=15mm
)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/0300dd89b97a107cf1140e89e79f18ef4feb9f0d.svg]]

#+begin_src jupyter-julia :session jl :async yes
f(xâ‚, xâ‚‚, xâ‚ƒ; Î², Ï‰â‚, Ï‰â‚‚, Ï‰â‚ƒ) = Î² + Ï‰â‚ * xâ‚ + Ï‰â‚‚ * xâ‚‚ + Ï‰â‚ƒ * xâ‚ƒ

Î²â‚, Î²â‚‚ = 0.5, 0.2
Ï‰â‚â‚, Ï‰â‚â‚‚, Ï‰â‚â‚ƒ = 1, 0.4, -0.3
Ï‰â‚‚â‚, Ï‰â‚‚â‚‚, Ï‰â‚‚â‚ƒ = 0.1, 0.1, 1.2

fâ‚(xâ‚, xâ‚‚, xâ‚ƒ) = f(xâ‚, xâ‚‚, xâ‚ƒ, Î²=Î²â‚, Ï‰â‚=Ï‰â‚â‚, Ï‰â‚‚=Ï‰â‚â‚‚, Ï‰â‚ƒ=Ï‰â‚â‚ƒ)
fâ‚‚(xâ‚, xâ‚‚, xâ‚ƒ) = f(xâ‚, xâ‚‚, xâ‚ƒ, Î²=Î²â‚‚, Ï‰â‚=Ï‰â‚‚â‚, Ï‰â‚‚=Ï‰â‚‚â‚‚, Ï‰â‚ƒ=Ï‰â‚‚â‚ƒ)

xâ‚, xâ‚‚, xâ‚ƒ = 4, -1, 2

yâ‚ = fâ‚(xâ‚, xâ‚‚, xâ‚ƒ)
yâ‚‚ = fâ‚‚(xâ‚, xâ‚‚, xâ‚ƒ)
println("Individual equations:")
@printf "yâ‚ = %f\nyâ‚‚ = %f\n\n" yâ‚ yâ‚‚

ð›ƒ = [Î²â‚; Î²â‚‚]
ð›€ = [Ï‰â‚â‚ Ï‰â‚â‚‚ Ï‰â‚â‚ƒ; Ï‰â‚‚â‚ Ï‰â‚‚â‚‚ Ï‰â‚‚â‚ƒ]
ð± = [xâ‚; xâ‚‚; xâ‚ƒ]

ð² = ð›ƒ + ð›€ * ð±
println("Matrix/vector form:")
@printf "yâ‚ = %f\nyâ‚‚ = %f" ð²[1] ð²[2]
#+end_src

#+RESULTS:
: Individual equations:
: yâ‚ = 3.500000
: yâ‚‚ = 2.900000
: 
: Matrix/vector form:
: yâ‚ = 3.500000
: yâ‚‚ = 2.900000

*1.* /A single linear equation with three inputs (i.e., =linear_function_3D()=) associates a value \( y \) with each point in a 3D space (\( x_1 \), \( x_2 \), \( x_3 \)). Is it possible to visualize this? What value is at position \( (0, 0, 0) \)?/

â€” Not with a static image, which is limited to three-dimensional representations. We would need four dimensions to visualize the three inputs \( x_1 \), \( x_2 \), \( x_3 \) and the output \( y \). The only way for this to be acomplished would be to use time as the fourth dimension, creating an animated image where one of the input variables changes with time. Otherwise, we would have to resort to dimensionality reduction techniques. The value of \( y \) at \( (0, 0, 0) \) is just \( \beta \), since all other terms are cancelled.

*2.* Write code to compute three linear equations with two inputs (\( x_1 \), \( x_2 \)) using both the individual equations and the matrix form (you can make up any values for the inputs \( \beta_i \) and the slopes \( \omega_{ij} \).

#+begin_src jupyter-julia :session jl :async yes
f(xâ‚, xâ‚‚; Î², Ï‰â‚, Ï‰â‚‚) = Î² + Ï‰â‚ * xâ‚ + Ï‰â‚‚ * xâ‚‚

fâ‚(xâ‚, xâ‚‚) = f(xâ‚, xâ‚‚, Î²=0, Ï‰â‚=1, Ï‰â‚‚=1)
fâ‚‚(xâ‚, xâ‚‚) = f(xâ‚, xâ‚‚, Î²=5, Ï‰â‚=0, Ï‰â‚‚=2)
fâ‚ƒ(xâ‚, xâ‚‚) = f(xâ‚, xâ‚‚, Î²=-5, Ï‰â‚=-2, Ï‰â‚‚=0)

f(ð±; Î², ð›š) = Î² + ð›š * ð±

ð›ƒ = [0, 5, -5]
ð›€ = [1 1; 0 2; -2 0]

plts = [
    heatmap(1:10, 1:10,
        (xâ‚, xâ‚‚) -> ð›ƒ[i] + ð›€[i, 1] * xâ‚ + ð›€[i, 2] * xâ‚‚,
        title  = L"y_%$i",
        xlabel = L"x_{%$(i)1}",
        ylabel = L"x_{%$(i)2}",
        fill   = true
    )
    for i âˆˆ axes(ð›€, 1)
]

plot(plts...,
    size=(1000, 350),
    margin=5mm,
    layout=(1, 3),
    legend=false,
    titlefontsize=16,
    guidefontsize=12
)
#+end_src

#+RESULTS:

[[file:./.ob-jupyter/61f588f71ef16f92c2696993f6b86be8d60214d0.svg]]

#+begin_src jupyter-julia :session jl :async yes
plot(exp, -5, 5,
    legend=false,
    xlabel=L"x",
    ylabel=L"\exp x",
    guidefontsize=16,
)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/8f3aeb3891dc529388fdfe30f41a9387c88099d4.svg]]

*1.* /What is \( \exp 0 \)?/

--- \( 1 \)

*2.* /What is \( \exp 1 \)?/

--- \( e \)

*3.* /What is \( \exp -\infty \)?/

--- \( 0 \)

*4.* /What is \( \exp +\infty \)?/

--- \( +\infty \)

*5.* /A function is convex if we can draw a straight line between any two points on the function, and the line lies above the function everywhere between these two points. Similarly, a function is concave if a straight line between any two points lies below the function everywhere between these two points. Is the exponential function convex or concave or neither?/

--- It is convex. Any straight line between two points on the \(\exp\) function is strictly above the function. The second derivative \(f''(x) = e^x = \exp x\) is strictly positive for all \(x \in \mathbb R\).

#+begin_src jupyter-julia :session jl :async yes
plot(log, -5, 10,
    xlabel=L"x",
    ylabel=L"\log x",
    legend=false,
    guidefontsize=16
)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/8a8871963434f1070d4059e44717884f1f383bb0.svg]]

*1.* /What is \(\log 0\)?/

--- Undefined. The logarithm function \(\log x\) is not defined for \(x \le 0\) in \(\mathbb R\)

*2.* /What is \(\log 1\)?/

--- \(0\)

*3.* /What is \(\log e\)?/

--- \(1\)

*4.* /What is \(\log(\exp 3)\)?/

--- \(3\)

*5.* /What is \(\exp(\log 4)\)?/

--- \(4\)

*6.* /What is \(\log(-1)\)/

--- Undefined.

*7.* /Is the logarithm function concave or convex?/

--- It is concave. Any straight line between two points on the \(\log\) function is strictly below the function. The second derivative \( f''(x) = -1/x^2 \) is strictly negative for all \( x \in \mathbb{R} \).

* Notebook 2.1 --- Supervised Learning

#+begin_src jupyter-julia :session jl :async yes
using LinearAlgebra, Plots, LaTeXStrings, Printf

pgfplotsx()
PGFPlotsX.CUSTOM_PREAMBLE = [raw"\usepackage{newpx}"]

default(
    guidefontsize=14,
    legendfontsize=12
)

ð± = [0.03, 0.19, 0.34, 0.46, 0.78, 0.81, 1.08, 1.18, 1.39, 1.60, 1.65, 1.90]
ð² = [0.67, 0.85, 1.05, 1.0, 1.40, 1.5, 1.3, 1.54, 1.55, 1.68, 1.73, 1.6]

f(ð±, ð›Ÿ) = muladd.(ð±, ð›Ÿ[2], ð›Ÿ[1])
L(ð›Ÿ) = sum(abs2, f(ð±, ð›Ÿ) .- ð²)
#+end_src

#+RESULTS:
: L (generic function with 1 method)

#+begin_src jupyter-julia :session jl :async yes
function compute_and_plot_loss(;ð›Ÿ)
    ð±â‚š = 0:2
    ð²â‚š = f(ð±â‚š, ð›Ÿ)

    println("L(Ï•â‚€=$(ð›Ÿ[1]), Ï•â‚=$(ð›Ÿ[2])) = $(L(ð›Ÿ))")
    plot(ð±â‚š, ð²â‚š,
        label=L"f_{\boldsymbol{\phi}}(x)",
        xlabel=L"x",
        ylabel=L"y"
    )
    scatter!(ð±, ð², label="Data")
end

compute_and_plot_loss(ð›Ÿ=[0.4, 0.2])
#+end_src

:RESULTS:
: L(Ï•â‚€=0.4, Ï•â‚=0.2) = 7.067864
[[file:./.ob-jupyter/5cb02ae2a5cdbf10ecd225020150beb99ac64b38.svg]]
:END:

#+begin_src jupyter-julia :session jl :async yes
compute_and_plot_loss(ð›Ÿ=[1.6, -0.8])
#+end_src

:RESULTS:
: L(Ï•â‚€=1.6, Ï•â‚=-0.8) = 10.279524

[[file:./.ob-jupyter/07fac69936ba4574f909e5b121a81ce74e30cccb.svg]]
:END:

#+begin_src jupyter-julia :session jl :async yes
compute_and_plot_loss(ð›Ÿ=[0.8, 0.5])
#+END_src

:RESULTS:
: L(Ï•â‚€=0.8, Ï•â‚=0.5) = 0.22852499999999987
[[file:./.ob-jupyter/eecc6c943abebcc3ca18f795284140a3acd5ba49.svg]]
:END:

#+begin_src jupyter-julia :session jl :async yes
heatmap(0.75:0.01:0.9, 0.45:0.01:0.6, (Ï•â‚€, Ï•â‚) -> L([Ï•â‚€, Ï•â‚]),
    clims=(0.2, 0.25),
    c=cgrad(:viridis, rev=true, scale=:exp),
    xlabel=L"\phi_0",
    ylabel=L"\phi_1"
)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/19af3c14f7f3f62fa4a91aa4c55cba3429f4148b.svg]]
* Problems 2.1--2.3 (Supervised Learning)

-----
*2.1.* /To walk â€œdownhillâ€ on the loss function (equation 2.5), we measure its gradient with respect to the parameters \( \phi_0 \) and \( \phi_1 \). Calculate expressions for the slopes \( \partial L / \partial \phi_0 \) and \( \partial L / \partial \phi_1 \)./
-----

\begin{align*}
\frac{\partial L}{\partial \phi_0} &= \frac{\partial}{\partial \phi_0} \sum_{i=1}^I (\mathrm{f}[x_i, \boldsymbol{\phi}] - y_i)^2 \\
&= \sum_{i=1}^I \frac{\partial}{\partial \phi_0} (\phi_0 + \phi_1 x_i - y_i)^2 \\
&= \sum 2(\phi_0 + \phi_1 x_i - y_i) \cdot \frac{\partial}{\partial{\phi_0}} (\phi_0 + \phi_1 x_i - y_i) \\
&= 2 \sum_{i=1}^I (\phi_0 + \phi_1 x_i - y_i)
\end{align*}

\begin{align*}
\frac{\partial L}{\partial \phi_1} &= \frac{\partial}{\partial \phi_1} \sum_{i=1}^I (\mathrm{f}[x_i, \boldsymbol{\phi}] - y_i)^2 \\
&= \sum_{i=1}^I \frac{\partial}{\partial \phi_1} (\phi_0 + \phi_1 x_i - y_i)^2 \\
&= \sum 2 (\phi_0 + \phi_1 x_i - y_i) \cdot \frac{\partial}{\partial \phi_1} (\phi_0 + \phi_1 x_i - y_i) \\
&= \sum 2 (\phi_0 + \phi_1 x_i - y_i) \cdot x_i \\
&= 2 \sum_{i=0}^I x_i (\phi_0 + \phi_1 x_i - y_i)
\end{align*}

-----
*2.2.* /Show that we can find the minimum of the loss function in closed form by setting the expression for the derivatives from problem 2.1 to zero and solving for \( \phi_0 \) and \( \phi_1 \). Note that this works for linear regression but not for more complex models; this is why we use iterative model fitting methods like gradient descent (figure 2.4)./
-----

\begin{align*}
\frac{\partial L}{\partial \phi_0} = {} & 2 \sum_{i=1}^I (\phi_0 + \phi_1 x_i - y_i) = 0 \\
\implies & I \phi_0 + \phi_1 \sum x_i - \sum y_i = 0 \\
\implies & \phi_0 = \frac{\sum y_i}{I} - \phi_1 \frac{\sum x_i}{I} \\
\implies & \phi_0 = \bar{y} - \phi_1 \bar{x}
\end{align*}

\begin{align*}
\frac{\partial L}{\partial \phi_1} = {} & 2 \sum_{i=1}^I x_i (\phi_0 + \phi_1 x_i - y_i) = 0 \\
\implies & \sum (\phi_1 x_i^2 + \phi_0 x_i - x_i y_i) = 0 \\
\implies & \phi_1 \sum  x_i^2 = \sum x_i y_i - \phi_0 \sum x_i \\
\implies & \phi_1 \sum  x_i^2 = \sum x_i y_i - (\bar{y} - \phi_1 \bar{x}) \sum x_i \\
\implies & \phi_1 \sum  x_i^2 = \sum x_i y_i - \bar{y} \sum x_i + \phi_1 \bar{x} \sum x_i \\
\implies & \phi_1 \left( \sum x_i^2 - \bar{x} \sum x_i \right) = \sum x_i y_i - \bar{y} \sum x_i \\
\implies & \phi_1 = \frac{\sum x_i y_i - \bar{y} \sum x_i}{\sum x_i^2 - \bar{x} \sum x_i} \\
\implies & \phi_1 = \frac{\sum x_i y_i - \bar{y} \sum x_i}{\sum x_i^2 - I \bar{x}^2} \\
\implies & \phi_1 = \frac{\sum x_i y_i - I \bar{x} \bar{y}}{\sum (x_i - \bar{x})^2} \\
\implies & \phi_1 = \frac{\sum_{i=1}^I (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^I (x_i - \bar{x})^2}
\end{align*}

\begin{align*}
\frac{\partial L}{\partial \phi_{1}} = {} & 2 \sum_{i=1}^I x_i (\phi_{0} + \phi_{1} x_i - y_i) = 0 \\
\implies & \sum (\phi_{1} x_i^{2} + \phi_{0} x_i - x_i y_i) = 0 \\
\implies & \phi_{1} \sum x_i^{2} = \sum x_i y_i - \phi_{0} \sum x_i \\
\implies & \phi_{1} \sum x_i^{2} = \sum x_i y_i - (\bar{y} - \phi_{1} \bar{x}) \sum x_i \\
\implies & \phi_{1} \sum x_i^{2} = \sum x_i y_i - \bar{y} \sum x_i + \phi_{1} \bar{x} \sum x_i \\
\implies & \phi_{1} \left( \sum x_i^{2} - \bar{x} \sum x_i \right) = \sum x_i y_i - \bar{y} \sum x_i \\
\implies & \phi_{1} = \frac{\sum x_i y_i - \bar{y} \sum x_i}{\sum x_i^{2} - \bar{x} \sum x_i} \\
\implies & \phi_{1} = \frac{\sum x_i y_i - \bar{y} \sum x_i}{\sum x_i^{2} - I \bar{x}^{2}} \\
\implies & \phi_{1} = \frac{\sum x_i y_i - I \bar{x} \bar{y}}{\sum (x_i - \bar{x})^{2}} \\
\implies & \phi_{1} = \frac{\sum_{i=1}^I (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^I (x_i - \bar{x})^{2}}
\end{align*}

\begin{align*}
\sum (x_i - \bar x)^2 &= \sum (x_i^2 - 2 x_i \bar x + \bar x^2) \\
&= \sum x_i^2 - 2 \bar x \sum x_i + I \bar x^2 \\
&= \sum x_i^2 - 2 \bar x(I\bar x) + I \bar x^2 \\
&= \sum x_i^2 - 2 I \bar x^2 + I \bar x^2 \\
&= \sum x_i^2 - I \bar x^2
\end{align*} 

\begin{align*}
\sum (x_i - \bar x) (y_i - \bar y) &= \sum (x_i y_i - x_i \bar y - \bar x y_i + \bar x \bar y) \\
&= \sum x_i y_i - \bar y \sum x_i - \bar x \sum y_i + I \bar x \bar y \\
&= \sum x_i y_i - \bar y (I \bar x) - \bar x (I \bar y) + I \bar x \bar y \\
&= \sum x_i y_i - I \bar x \bar y
\end{align*}

-----
*2.3.* /Consider reformulating linear regression as a generative model, so we have \( x = \mathrm{g}[y, \boldsymbol{\phi}] = \phi_{0} + \phi_{1} y \). What is the new loss function? Find an expression for the inverse function \( y = \mathrm{g}^{-1}[x, \boldsymbol{\phi}] \) that we would use to perform inference. Will this model make the same predictions as the discriminative version for a given training dataset \( \{x_i, y_i\} \)? One way to establish this is to write code that fits a line to three data points using both methods and see if the result is the same./
-----

* Problems 3.1--3.8 (Shallow Neural Networks)

-----
*3.1.* /What kind of mapping from input to output would be created if the activation function in equation 3.1 was linear so that \( \mathrm{a}[z] = \psi_0 + \psi_1 z \)? What kind of mapping would be created if the activation function was removed, so \( \mathrm{a}[z] = z \)?/
-----

The equation resulting from a linear activation would be as follows:

\begin{align*}
\mathrm{a}[z] &= \psi_0 + \psi_1 z \\
&= \psi_0 + \psi_1 (\theta_0 + \theta_1 x) \\
&= \psi_0 + \psi_1 \theta_0 + \psi_1 \theta_1 x.
\end{align*}

Let \( \theta_0' = \psi_0 + \psi_1 \theta_0 \) and \( \theta_1' = \psi_1 \theta_1 \). Since \( \theta'_0 \) and \( \theta'_1 \) are constants, the result is a linear function:

\begin{align*}
\mathrm{a}[z] = \theta'_0 + \theta'_1 z.
\end{align*}

So the result is the same as having no activation function, only with modified parameters. If this is the case, each hidden unit becomes a regular linear function. When these are weighted and summed, the final equation is also linear.

In conclusion, a nonlinear activation function is necessary to â€œbreakâ€ the functionâ€™s linearity.

-----
*3.2* /For each of the four linear regions in figure 3.3j, indicate which hidden units are inactive and which are active (i.e., which do and do not clip their inputs)./
-----

+ Region 1: \( h_3 \)
+ Region 2: \( h_1 \), \( h_3 \)
+ Region 3: \( h_1 \), \( h_2 \), \( h_3 \)
+ Region 4: \( h_1 \), \( h_2 \)

-----
*3.3* /Derive expressions for the positions of the â€œjointsâ€ in function in figure 3.3j in terms of the ten parameters \( \boldsymbol{\phi} \) and the input \( x \). Derive expressions for the slopes of the four linear regions./
-----

The â€œjointsâ€ take place where the linear functions pass through 0, which is where the ReLU â€œcutâ€ happens, resulting in the four distinct slopes:

\begin{align*}
\theta_0 + \theta_1 x = 0 \\
x = \frac{-\theta_0}{\theta_1}.
\end{align*}

Therefore, the 3 division points on the x-axis are:

\begin{align*}
x_1 = \frac{-\theta_{10}}{\theta_{11}}
&& x_2 = \frac{-\theta_{20}}{\theta_{21}}
&& x_3 = \frac{-\theta_{30}}{\theta_{31}}.
\end{align*}

As for the slopes, we simply sum the slopes from the linear functions of each active hidden unit, multiplied by their respective weights:

\begin{align*}
\text{S}_1 &= \phi_3 \theta_{31} \\
\text{S}_2 &= \phi_1 \theta_{11} + \phi_3 \theta_{31} \\
\text{S}_3 &= \phi_1 \theta_{11} + \phi_2 \theta_{21} + \phi_3 \theta_{31} \\
\text{S}_4 &= \phi_1 \theta_{11} + \phi_2 \theta_{21}.
\end{align*}

-----
*3.4* /Draw a version of figure 3.3 where the y-intercept and slope of the third hidden unit have changed as in figure 3.14c. Assume that the remaining parameters remain the same./
-----

#+begin_src jupyter-julia :session jl :async yes
using Plots, LaTeXStrings

pgfplotsx()
PGFPlotsX.CUSTOM_PREAMBLE = [raw"\usepackage{newpx}"]

fâ‚(x) = -0.2 + 0.4x
fâ‚‚(x) = -0.9 + 0.9x
fâ‚ƒ(x) = -1.1 + 0.7x
a(x) = max(0, x)

plot(
    plot(fâ‚ƒ, 0, 2, title=L"\theta_{30} + \theta_{31}x"),
    plot(x -> a(f(x)), 0, 2, title=L"h_3 = \mathrm{a}[\theta_{30} + \theta_{31}x]"),
    plot(x -> 0.7 * a(fâ‚ƒ(x)), 0, 2, title=L"\phi_3 h_3"),
    plot(x -> -1.2 * a(fâ‚(x)) + 1.1 * a(fâ‚‚(x)) + 0.7 * a(fâ‚ƒ(x)), 0, 2, title=L"\phi_1 h_1 + \phi_2 h_2 + \phi_3 h_3"),
    ylims=(-1, 1),
    lw=2,
    legend=false,
)
#+end_src

#+RESULTS:

[[file:./.ob-jupyter/b623a71ccdd55ed85e2139731fa4979aa924e227.svg]]

-----
*3.5* /Prove that the following property holds for \( \alpha \in \mathbb{R}^+ \):/

\begin{align*}
\textrm{ReLU}[\alpha \cdot z] = \alpha \cdot \textrm{ReLU}[z].
\end{align*}

/This is known as the non-negative homogeneity property of the ReLU function./
-----

\begin{align*}
\textrm{ReLU}[\alpha \cdot z] &= \max(0, \alpha \cdot z) & (\text{by definition}) \\
&= \max(\alpha \cdot 0, \alpha \cdot z) & (\text{since } \alpha \cdot 0 = 0) \\
&= \alpha \cdot \max(0, z) & (\text{since } \alpha > 0 \text{ allows factoring}) \\
&= \alpha \cdot \textrm{ReLU}[z] .
\end{align*}

-----
*3.6* /Following on from problem 3.5, what happens to the shallow network defined in equations 3.3 and 3.4 when we multiply the parameters \( \theta_{10} \) and \( \theta_{11} \) by a positive constant \( \alpha \) and divide the slope \( \phi_1 \) by the same parameter \( \alpha \)? What happens if \( \alpha \) is negative?/
-----

Multiplying \( \theta_{10} \) and \( \theta_{11} \) by a constant \( \alpha \) is equivalent to multiplying the entire linear function by this constant:

\begin{align*}
\alpha \theta_{10} + \alpha \theta_{11} x = \alpha(\theta_{10} + \theta_{11} x).
\end{align*}

Given the non-negative homogenity property of the ReLU function (showed on problem 3.5), it follows that \( \mathrm{a}[\alpha \cdot z] = \alpha \cdot \mathrm{a}[z] \) and, thus:

\begin{align*}
\mathrm{a} [\alpha (\theta_{10} + \theta_{11} x)] &= \alpha \cdot \mathrm{a} [\theta_{10} + \theta_{11} x] \\
&= \alpha h_1.
\end{align*}

This term is then multiplied by \( \phi_1 \), which if divided by the same constant \( \alpha \), results in the original slope:

\begin{align*}
\frac{\phi_1}{\alpha} \alpha h_1 = \phi_1 h_1.
\end{align*}

If \( \alpha \) is negative, multiplying \( \theta_{1\bullet} \) by it will cause the original linear function to flip on the x-axis, consequently activating the previously inactive area and vice-versa:

\begin{align*}
\mathrm{ReLU}[\alpha \cdot z] &= \max(0, \alpha \cdot z) \\
&= \max(0, -|\alpha| \cdot z) & (\text{given } \alpha < 0) \\
&= -\min(0, |\alpha| \cdot z) \\
&= -|\alpha| \cdot \min(0, z) \\
&= \alpha \cdot \min(0, z).
\end{align*}

In this case, dividing the slope by \( \alpha \) will undo the scaling and flip the active region on the x-axis.

Therefore, the overall effect of these operations when \( \alpha \) is negative is swapping the active and inactive regions of the hidden unit, creating a negative-only rectifier. That is:

\begin{align*}
\frac{\phi_1}{\alpha} \cdot \textrm{ReLU}[\alpha \cdot z] &= \frac{\phi_1}{\alpha} \cdot \alpha \min(0, z) & (\text{given } \alpha < 0) \\
&= \phi_1 \min(0, z).
\end{align*}

-----
*3.7* /Consider fitting the model in equation 3.1 using a least squares loss function. Does this loss function have a unique minimum? i.e., is there a single â€œbestâ€ set of parameters?/
-----

The loss function cannot have a unique minumum. Even if the data can be perfectly described by a 4-region piecewise linear function, the hidden units can always be swapped without altering the final function.

-----
*3.8* /Consider replacing the ReLU activation function with (i) the Heaviside step function \( \textrm{heaviside}[z] \), (ii) the hyperbolic tangent function \( \mathrm{tanh}[z] \), and (iii) the rectangular function \( \mathrm{rect}[z] \), where:/

\begin{align*}
\textrm{heaviside}[z] = \begin{cases}
0, & z < 0 \\
1, & z \ge 0
\end{cases} &&
\textrm{rect}[z] = \begin{cases}
0, & z < 0 \\
1, & 0 \le z \le 1 \\
0, & z > 1.
\end{cases}
\end{align*}

/Redraw a version of figure 3.3 for each of these functions. The original parameters were:/

\begin{align*}
\boldsymbol{\phi} &= \{ \phi_0, \phi_1, \phi_2, \phi_3, \phi_{10}, \phi_{11}, \phi_{20}, \phi_{21}, \phi_{30}, \phi_{31} \} \\
&= \{ -0.23, -1.3, 1.3, 0.66, -0.2, 0.4, -0.9, 0.9, 1.1, -0.7 \}.
\end{align*}

/Provide an informal description of the family of functions that can be created by neural networks with one input, three hidden units, and one output for each activation function./
-----

#+begin_src jupyter-julia :session jl :async yes
using Plots, LaTeXStrings

pgfplotsx()
PGFPlotsX.CUSTOM_PREAMBLE = [raw"\usepackage{newpx}"]

x = 0:0.01:2
Ï• = [-0.23, -1.3, 1.3, 0.66]
Î¸ = [-0.2 -0.9 1.1
     0.4 0.9 -0.7]

lâ‚(x) = Î¸[1, 1] + Î¸[2, 1] * x
lâ‚‚(x) = Î¸[1, 2] + Î¸[2, 2] * x
lâ‚ƒ(x) = Î¸[1, 3] + Î¸[2, 3] * x

function plot_activation(a, plot_title)
    f(x) = Ï•[1] + Ï•[2] * a(lâ‚(x)) + Ï•[3] * a(lâ‚‚(x)) + Ï•[4] * a(lâ‚ƒ(x))

    fig_data = [
        (x -> lâ‚(x), L"\theta_{10} + \theta_{11} x"),
        (x -> lâ‚‚(x), L"\theta_{20} + \theta_{21} x"),
        (x -> lâ‚ƒ(x), L"\theta_{30} + \theta_{31} x"),
        (x -> a(lâ‚(x)), L"h_1 = \mathrm{a}[\theta_{10} + \theta_{11} x]"),
        (x -> a(lâ‚‚(x)), L"h_2 = \mathrm{a}[\theta_{20} + \theta_{21} x]"),
        (x -> a(lâ‚ƒ(x)), L"h_3 = \mathrm{a}[\theta_{30} + \theta_{31} x]"),
        (x -> Ï•[1] * a(lâ‚(x)), L"\phi_1 h_1"),
        (x -> Ï•[2] * a(lâ‚‚(x)), L"\phi_2 h_2"),
        (x -> Ï•[3] * a(lâ‚ƒ(x)), L"\phi_3 h_3"),
        (x -> f(x), L"\phi_0 + \phi_1 h_1 + \phi_2 h_2 + \phi_3 h_3")
    ]
    subplots = map(fig_data) do (f, title)
        plot(f, x, title=title)
    end

    plot(
        subplots...,
        legend=false,
        layout=(4, 3),
        size=(900, 1200),
        ylims=(-1, 1),
        xlabel=L"x",
        ylabel=L"y",
        plot_title=plot_title
    )
end
#+end_src

#+RESULTS:
: plot_activation (generic function with 1 method)

#+begin_src jupyter-julia :session jl :async yes
ReLU(x) = max(0, x)
plot_activation(ReLU, "ReLU")
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/cd2c9af339ced413fc2fcf7cc0ef17ace7ddd77c.svg]]

/ReLU:/ Piecewise linear functions with 4 linear regions.

#+begin_src jupyter-julia :session jl :async yes
heaviside(x) = x < 0 ? 0 : 1
plot_activation(heaviside, "heaviside")
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/6eea80b6641b23c14ef3fafb80a172d4fa058c26.svg]]

/heaviside:/ Piecewise â€œconstantâ€ functions with 4 constant regions.

#+begin_src jupyter-julia :session jl :async yes
plot_activation(tanh, "tanh")
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/7be2d0691d5079435d4191c3dfa7a51c46b805ce.svg]]

/tanh:/ Piecewise hyperbolic functions with 4 regions. Perhaps a â€œsmoothed-outâ€ version of the /heaviside/ function, where the transition is a hyperbola instead of a direct jump.

#+begin_src jupyter-julia :session jl :async yes
rect(x) = (0 <= x <= 1) ? 1 : 0
plot_activation(rect, "rect")
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/c0e22aee34cde951dc5bd6e9636210fd6fecccec.svg]]

/rect:/ Piecewise â€œconstantâ€ functions with 7 constant regions.

* Notebook 3.1 --- Shallow Neural Networks I

#+begin_src jupyter-julia :session jl :async yes
using Plots, LaTeXStrings

pgfplotsx()
PGFPlotsX.CUSTOM_PREAMBLE = [raw"\usepackage{newpx}"]

ReLU(x) = max(0, x)
#+end_src

#+RESULTS:
: ReLU (generic function with 1 method)

#+begin_src jupyter-julia :session jl :async yes
plot(ReLU, -5:0.1:5,
    ylims=(-5, 5),
    xlabel=L"z",
    ylabel=L"\mathrm{ReLU}[z]",
    legend=false
)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/40aabd4a02c97b6bd26c19afbd1570ea6a58da87.svg]]

\begin{align*}
\mathbf{b}_{\mathrm{in}} &\gets \boldsymbol{\Theta}_{1 \bullet} \\
\mathbf{w}_{\mathrm{in}} &\gets \boldsymbol{\Theta}_{2 \bullet} \\
b_{\mathrm{out}} &\gets \boldsymbol{\phi}_1 \\
\mathbf{w}_{\mathrm{out}} &\gets \boldsymbol{\phi}_{2:k}
\end{align*}

\begin{align*}
\mathbf{Z} &\gets \mathbf{w}_{\mathrm{in}} \mathbf{x}^{\top} + \mathbf{b}_{\mathrm{in}} \mathbf{1}_N^{\top} \\
\mathbf{H} &\gets \mathrm{a}(\mathbf{Z}) \\
\mathbf{H}_w &\gets \mathrm{diag}( \mathbf{w}_{\mathrm{out}} ) \mathbf{H}  \\
\mathbf{y} &\gets \mathbf{1}_N^{\top} b_{\mathrm{out}} + \mathbf{1}_K^{\top} \mathbf{H}_w\\
\\
\text{or} \\
\\
\mathbf{Z} &\gets \mathbf{1}_N \mathbf{b}_{\mathrm{in}}^{\mathrm{\top}} + \mathbf{x} \mathbf{w}_{\mathrm{in}}^{\top} \\
\mathbf{H} &\gets \mathrm{a}(\mathbf{Z}) \\
\mathbf{y} &\gets \mathbf{1}_N b_{\mathrm{out}} + \mathbf{H} \mathbf{w}_{\mathrm{out}}
\end{align*}

\begin{align*}
\mathbf{y} = \mathbf{1}_{N} b_{\mathrm{out}} + \mathrm{a} \! \left( \mathbf{1}_N \mathbf{b}_{\mathrm{in}}^{\top} + \mathbf{x} \mathbf{w}_{\mathrm{in}}^{\top} \right) \! \mathbf{w}_{\mathrm{out}}
\end{align*}

#+begin_src jupyter-julia :session jl :async yes
function shallow_nn(x, a, Ï•, Î˜)
    b_in = Î˜[:, 1]
    w_in = Î˜[:, 2]
    b_out = Ï•[1]
    w_out = Ï•[2:end]

    Z = (w_in * x') .+ b_in
    H = a.(Z)
    H_w = H .* w_out
    y = sum(H_w, dims=1) .+ b_out

    return vec(y), Z, H, H_w
end

function plot_nn(x, y, Z, H, H_w; plot_all=false, x_data=nothing, y_data=nothing)
    if plot_all
        fig_data = [
            (x, Z[1, :], L"\theta_{10} + \theta_{11} x"),
            (x, Z[2, :], L"\theta_{20} + \theta_{21} x"),
            (x, Z[3, :], L"\theta_{30} + \theta_{31} x"),
            (x, H[1, :], L"h_1 = \mathrm{a}[\theta_{10} + \theta_{11} x]"),
            (x, H[2, :], L"h_2 = \mathrm{a}[\theta_{20} + \theta_{21} x]"),
            (x, H[3, :], L"h_3 = \mathrm{a}[\theta_{30} + \theta_{31} x]"),
            (x, H_w[1, :], L"\phi_1 h_1"),
            (x, H_w[2, :], L"\phi_2 h_2"),
            (x, H_w[3, :], L"\phi_3 h_3"),
            (x, y, L"\phi_0 + \phi_1 h_1 + \phi_2 h_2 + \phi_3 h_3")
        ]
        subplots = map(fig_data) do (x, y, title)
            plot(x, y,
                title=title, ylims=(-1, 1), xlabel=L"x", ylabel=L"y", legend=false
            )
        end
        if !isnothing(x_data) && !isnothing(y_data)
            scatter!(subplots[end], x_data, y_data,
                markersize=2, markerstrokewidth=0, color=:blueviolet
            )
        end
        plot(subplots..., layout=(4, 3), size=(600, 800))
    else
        p = plot(x, y,
            legend=false, ylims=(-1, 1), xlabel=L"x", ylabel=L"y",
            title=L"\phi_0 + \phi_1 h_1 + \phi_2 h_2 + \phi_3 h_3"
        )
        if !isnothing(x_data) && !isnothing(y_data)
            scatter!(p, x_data, y_data,
                markersize=2, markerstrokewidth=0, color=:blueviolet
            )
        end
        return p
    end
end

Î˜ = [0.3 -1.0
     -1.0 2.0
     -0.5 0.65]
Ï• = [-0.3, 2.0, -1.0, 7.0]
x = 0:0.01:1

y, Z, H, H_w = shallow_nn(x, ReLU, Ï•, Î˜)
plot_nn(x, y, Z, H, H_w; plot_all=true)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/5265122aba8a35960c5d391fc00be2dd9f169f16.svg]]

#+begin_src jupyter-julia :session jl :async yes
using Printf

x_train = [0.09291784, 0.46809093, 0.93089486, 0.67612654, 0.73441752, 0.86847339, 0.49873225, 0.51083168, 0.18343972, 0.99380898, 0.27840809, 0.38028817, 0.12055708, 0.56715537, 0.92005746, 0.77072270, 0.85278176, 0.05315950, 0.87168699, 0.58858043]
y_train = [-0.15934537, 0.18195445, 0.451270150, 0.13921448, 0.09366691, 0.30567674, 0.372291170, 0.40716968, -0.08131792, 0.41187806, 0.36943738, 0.3994327, 0.019062570, 0.35820410, 0.452564960, -0.0183121, 0.02957665, -0.24354444, 0.148038840,0.26824970]

least_squares_loss(y, Å·) = sum(abs2, y .- Å·)

Î˜ = [0.3 -1.0
     -1.1 2.0
     -0.5 0.65]
Ï• = [0.4, -2.5, -1.0, 7.0]

y_predict, Z, H, H_w = shallow_nn(x_train, ReLU, Ï•, Î˜)

loss = least_squares_loss(y_train, y_predict)
@printf("Your Loss = %.3f, True value = 9.385", loss)

y, Z, H, H_w = shallow_nn(x, ReLU, Ï•, Î˜)
plot_nn(x, y, Z, H, H_w; plot_all=true, x_data=x_train, y_data=y_train)
#+end_src

#+RESULTS:
:RESULTS:
: Your Loss = 0.166, True value = 9.385
[[file:./.ob-jupyter/ef58c083afd2c98eeb242e17e3d7c1b664c20d43.svg]]
:END:

* Problems 3.9--3.13 (Shallow Neural Networks)

-----
*3.9* /Show that the third linear region in figure 3.3 has a slope that is the sum of the slopes of the first and fourth linear regions./
-----

As seen in exercise 3.3, these are the slopes for each of the 4 linear regions:

\begin{align*}
\textrm{S}_1 &= \phi_3 \theta_{31} \\
\textrm{S}_2 &= \phi_1 \theta_{11} + \phi_3 \theta_{31} \\
\textrm{S}_3 &= \phi_1 \theta_{11} + \phi_2 \theta_{21} + \phi_3 \theta_{31} \\
\textrm{S}_4 &= \phi_1 \theta_{11} + \phi_2 \theta_{21}.
\end{align*}

It is easy to see that the slope for \( S_3 \) is composed by \( S_1 \) and \( S_4 \). That is,

\begin{align*}
\textrm{S}_3 &= S_3 + S_1.
\end{align*}

-----
*3.10* /Consider a neural network with one input, one output, and three hidden units. The construction in figure 3.3 shows how this creates four linear regions. Under what circumstances could this network produce a function with fewer than four linear regions?/
-----

When multiple activations occur at the same point in the x-axis or when one of the non-intercept terms is 0.

-----
*3.11* /How many parameters does the model in figure 3.6 have?/
-----

18. \( \theta_{10} \), \( \theta_{11} \), \( \theta_{20} \), \( \theta_{21} \), \( \theta_{30} \), \( \theta_{31} \), \( \theta_{40} \), \( \theta_{41} \), \( \phi_{10} \), \( \phi_{11} \), \( \phi_{12} \), \( \phi_{13} \), \( \phi_{14} \), \( \phi_{20} \), \( \phi_{21} \), \( \phi_{22} \), \( \phi_{23} \), \( \phi_{24} \).

        -----
*3.12* /How many parameters does the model in figure 3.7 have?/
-----

13. \( \theta_{10} \), \( \theta_{11} \), \( \theta_{12} \), \( \theta_{20} \), \( \theta_{21} \), \( \theta_{22} \), \( \theta_{30} \), \( \theta_{31} \), \( \theta_{32} \), \( \phi_0 \), \( \phi_1 \), \( \phi_2 \), \( \phi_3 \).

        -----
*3.13* /What is the activation pattern for each of the seven regions in figure 3.8? In other words, which hidden units are active (pass the input) and which are inactive (clip the input) for each region?/
-----

Starting from the largest region and moving clockwise, internally:
1. \( h_3 \)
2. None.
3. \( h_1 \)
4. \( h_1, h_2 \)
5. \( h_1, h_2, h_3 \)
6. \( h_2, h_3 \)
7. \( h_1 \)
