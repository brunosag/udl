* Notebook 1.1 --- Background Mathematics

#+begin_src jupyter-julia :session jl :async yes
using Plots, LaTeXStrings, Measures, Printf, LinearAlgebra

pgfplotsx()
PGFPlotsX.CUSTOM_PREAMBLE = [raw"\usepackage{newpx}"]
#+end_src

#+RESULTS:
: 1-element Vector{String}:
:  "\\usepackage{newpx}"

#+begin_src jupyter-julia :session jl :async yes
f(x; Œ≤=0, œâ=1) = Œ≤ + œâ * x

p = plot(x -> f(x, Œ≤=10, œâ=-2), 0:10,
    xlims=(0, 10),
    ylims=(0, 10),
    xlabel=L"x",
    ylabel=L"f(x)",
    title="1D linear function",
    legend=false,
    titlefontsize = 20,
    guidefontsize = 20,
    tickfontsize = 14,
    background_color=:transparent,
    background_color_inside=:transparent
)

io = IOBuffer()
show(io, MIME("image/svg+xml"), p)
svg_content = String(take!(io))
cleaned_svg = replace(svg_content, r"<path[^>]*fill=\"rgb\(100%, 100%, 100%\)\"[^>]*/>" => "")
display("image/svg+xml", cleaned_svg)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/c3a14894398e4b96f5121a06bf7a193fb9c726ee.svg]]

#+begin_src jupyter-julia :session jl :async yes
f(x1, x2; Œ≤, œâ1, œâ2) = Œ≤ + œâ1 * x1 + œâ2 * x2
f‚Ä≤(x1, x2) = f(x1, x2, Œ≤=0, œâ1=1, œâ2=-0.5)

p_sf = surface(1:10, 1:10, f‚Ä≤,
    zlabel=L"y",
    guidefontsize=14
)
p_ct = contour(1:10, 1:10, f‚Ä≤,
    guidefontsize=18
)

plot(p_sf, p_ct,
    legend=false,
    title="2D linear function",
    xlabel=L"x_1",
    ylabel=L"x_2",
    size=(1000, 500),
    margin=15mm
)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/0300dd89b97a107cf1140e89e79f18ef4feb9f0d.svg]]

#+begin_src jupyter-julia :session jl :async yes
f(x‚ÇÅ, x‚ÇÇ, x‚ÇÉ; Œ≤, œâ‚ÇÅ, œâ‚ÇÇ, œâ‚ÇÉ) = Œ≤ + œâ‚ÇÅ * x‚ÇÅ + œâ‚ÇÇ * x‚ÇÇ + œâ‚ÇÉ * x‚ÇÉ

Œ≤‚ÇÅ, Œ≤‚ÇÇ = 0.5, 0.2
œâ‚ÇÅ‚ÇÅ, œâ‚ÇÅ‚ÇÇ, œâ‚ÇÅ‚ÇÉ = 1, 0.4, -0.3
œâ‚ÇÇ‚ÇÅ, œâ‚ÇÇ‚ÇÇ, œâ‚ÇÇ‚ÇÉ = 0.1, 0.1, 1.2

f‚ÇÅ(x‚ÇÅ, x‚ÇÇ, x‚ÇÉ) = f(x‚ÇÅ, x‚ÇÇ, x‚ÇÉ, Œ≤=Œ≤‚ÇÅ, œâ‚ÇÅ=œâ‚ÇÅ‚ÇÅ, œâ‚ÇÇ=œâ‚ÇÅ‚ÇÇ, œâ‚ÇÉ=œâ‚ÇÅ‚ÇÉ)
f‚ÇÇ(x‚ÇÅ, x‚ÇÇ, x‚ÇÉ) = f(x‚ÇÅ, x‚ÇÇ, x‚ÇÉ, Œ≤=Œ≤‚ÇÇ, œâ‚ÇÅ=œâ‚ÇÇ‚ÇÅ, œâ‚ÇÇ=œâ‚ÇÇ‚ÇÇ, œâ‚ÇÉ=œâ‚ÇÇ‚ÇÉ)

x‚ÇÅ, x‚ÇÇ, x‚ÇÉ = 4, -1, 2

y‚ÇÅ = f‚ÇÅ(x‚ÇÅ, x‚ÇÇ, x‚ÇÉ)
y‚ÇÇ = f‚ÇÇ(x‚ÇÅ, x‚ÇÇ, x‚ÇÉ)
println("Individual equations:")
@printf "y‚ÇÅ = %f\ny‚ÇÇ = %f\n\n" y‚ÇÅ y‚ÇÇ

ùõÉ = [Œ≤‚ÇÅ; Œ≤‚ÇÇ]
ùõÄ = [œâ‚ÇÅ‚ÇÅ œâ‚ÇÅ‚ÇÇ œâ‚ÇÅ‚ÇÉ; œâ‚ÇÇ‚ÇÅ œâ‚ÇÇ‚ÇÇ œâ‚ÇÇ‚ÇÉ]
ùê± = [x‚ÇÅ; x‚ÇÇ; x‚ÇÉ]

ùê≤ = ùõÉ + ùõÄ * ùê±
println("Matrix/vector form:")
@printf "y‚ÇÅ = %f\ny‚ÇÇ = %f" ùê≤[1] ùê≤[2]
#+end_src

#+RESULTS:
: Individual equations:
: y‚ÇÅ = 3.500000
: y‚ÇÇ = 2.900000
: 
: Matrix/vector form:
: y‚ÇÅ = 3.500000
: y‚ÇÇ = 2.900000

*1.* /A single linear equation with three inputs (i.e., =linear_function_3D()=) associates a value \( y \) with each point in a 3D space (\( x_1 \), \( x_2 \), \( x_3 \)). Is it possible to visualize this? What value is at position \( (0, 0, 0) \)?/

‚Äî Not with a static image, which is limited to three-dimensional representations. We would need four dimensions to visualize the three inputs \( x_1 \), \( x_2 \), \( x_3 \) and the output \( y \). The only way for this to be acomplished would be to use time as the fourth dimension, creating an animated image where one of the input variables changes with time. Otherwise, we would have to resort to dimensionality reduction techniques. The value of \( y \) at \( (0, 0, 0) \) is just \( \beta \), since all other terms are cancelled.

*2.* Write code to compute three linear equations with two inputs (\( x_1 \), \( x_2 \)) using both the individual equations and the matrix form (you can make up any values for the inputs \( \beta_i \) and the slopes \( \omega_{ij} \).

#+begin_src jupyter-julia :session jl :async yes
f(x‚ÇÅ, x‚ÇÇ; Œ≤, œâ‚ÇÅ, œâ‚ÇÇ) = Œ≤ + œâ‚ÇÅ * x‚ÇÅ + œâ‚ÇÇ * x‚ÇÇ

f‚ÇÅ(x‚ÇÅ, x‚ÇÇ) = f(x‚ÇÅ, x‚ÇÇ, Œ≤=0, œâ‚ÇÅ=1, œâ‚ÇÇ=1)
f‚ÇÇ(x‚ÇÅ, x‚ÇÇ) = f(x‚ÇÅ, x‚ÇÇ, Œ≤=5, œâ‚ÇÅ=0, œâ‚ÇÇ=2)
f‚ÇÉ(x‚ÇÅ, x‚ÇÇ) = f(x‚ÇÅ, x‚ÇÇ, Œ≤=-5, œâ‚ÇÅ=-2, œâ‚ÇÇ=0)

f(ùê±; Œ≤, ùõö) = Œ≤ + ùõö * ùê±

ùõÉ = [0, 5, -5]
ùõÄ = [1 1; 0 2; -2 0]

plts = [
    heatmap(1:10, 1:10,
        (x‚ÇÅ, x‚ÇÇ) -> ùõÉ[i] + ùõÄ[i, 1] * x‚ÇÅ + ùõÄ[i, 2] * x‚ÇÇ,
        title  = L"y_%$i",
        xlabel = L"x_{%$(i)1}",
        ylabel = L"x_{%$(i)2}",
        fill   = true
    )
    for i ‚àà axes(ùõÄ, 1)
]

plot(plts...,
    size=(1000, 350),
    margin=5mm,
    layout=(1, 3),
    legend=false,
    titlefontsize=16,
    guidefontsize=12
)
#+end_src

#+RESULTS:

[[file:./.ob-jupyter/61f588f71ef16f92c2696993f6b86be8d60214d0.svg]]

#+begin_src jupyter-julia :session jl :async yes
plot(exp, -5, 5,
    legend=false,
    xlabel=L"x",
    ylabel=L"\exp x",
    guidefontsize=16,
)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/8f3aeb3891dc529388fdfe30f41a9387c88099d4.svg]]

*1.* /What is \( \exp 0 \)?/

--- \( 1 \)

*2.* /What is \( \exp 1 \)?/

--- \( e \)

*3.* /What is \( \exp -\infty \)?/

--- \( 0 \)

*4.* /What is \( \exp +\infty \)?/

--- \( +\infty \)

*5.* /A function is convex if we can draw a straight line between any two points on the function, and the line lies above the function everywhere between these two points. Similarly, a function is concave if a straight line between any two points lies below the function everywhere between these two points. Is the exponential function convex or concave or neither?/

--- It is convex. Any straight line between two points on the \(\exp\) function is strictly above the function. The second derivative \(f''(x) = e^x = \exp x\) is strictly positive for all \(x \in \mathbb R\).

#+begin_src jupyter-julia :session jl :async yes
plot(log, -5, 10,
    xlabel=L"x",
    ylabel=L"\log x",
    legend=false,
    guidefontsize=16
)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/8a8871963434f1070d4059e44717884f1f383bb0.svg]]

*1.* /What is \(\log 0\)?/

--- Undefined. The logarithm function \(\log x\) is not defined for \(x \le 0\) in \(\mathbb R\)

*2.* /What is \(\log 1\)?/

--- \(0\)

*3.* /What is \(\log e\)?/

--- \(1\)

*4.* /What is \(\log(\exp 3)\)?/

--- \(3\)

*5.* /What is \(\exp(\log 4)\)?/

--- \(4\)

*6.* /What is \(\log(-1)\)/

--- Undefined.

*7.* /Is the logarithm function concave or convex?/

--- It is concave. Any straight line between two points on the \(\log\) function is strictly below the function. The second derivative \( f''(x) = -1/x^2 \) is strictly negative for all \( x \in \mathbb{R} \).

* Notebook 2.1 --- Supervised Learning

#+begin_src jupyter-julia :session jl :async yes
using LinearAlgebra, Plots, LaTeXStrings, Printf

pgfplotsx()
PGFPlotsX.CUSTOM_PREAMBLE = [raw"\usepackage{newpx}"]

default(
    guidefontsize=14,
    legendfontsize=12
)

ùê± = [0.03, 0.19, 0.34, 0.46, 0.78, 0.81, 1.08, 1.18, 1.39, 1.60, 1.65, 1.90]
ùê≤ = [0.67, 0.85, 1.05, 1.0, 1.40, 1.5, 1.3, 1.54, 1.55, 1.68, 1.73, 1.6]

f(ùê±, ùõü) = muladd.(ùê±, ùõü[2], ùõü[1])
L(ùõü) = sum(abs2, f(ùê±, ùõü) .- ùê≤)
#+end_src

#+RESULTS:
: L (generic function with 1 method)

#+begin_src jupyter-julia :session jl :async yes
function compute_and_plot_loss(;ùõü)
    ùê±‚Çö = 0:2
    ùê≤‚Çö = f(ùê±‚Çö, ùõü)

    println("L(œï‚ÇÄ=$(ùõü[1]), œï‚ÇÅ=$(ùõü[2])) = $(L(ùõü))")
    plot(ùê±‚Çö, ùê≤‚Çö,
        label=L"f_{\boldsymbol{\phi}}(x)",
        xlabel=L"x",
        ylabel=L"y"
    )
    scatter!(ùê±, ùê≤, label="Data")
end

compute_and_plot_loss(ùõü=[0.4, 0.2])
#+end_src

:RESULTS:
: L(œï‚ÇÄ=0.4, œï‚ÇÅ=0.2) = 7.067864
[[file:./.ob-jupyter/5cb02ae2a5cdbf10ecd225020150beb99ac64b38.svg]]
:END:

#+begin_src jupyter-julia :session jl :async yes
compute_and_plot_loss(ùõü=[1.6, -0.8])
#+end_src

:RESULTS:
: L(œï‚ÇÄ=1.6, œï‚ÇÅ=-0.8) = 10.279524

[[file:./.ob-jupyter/07fac69936ba4574f909e5b121a81ce74e30cccb.svg]]
:END:

#+begin_src jupyter-julia :session jl :async yes
compute_and_plot_loss(ùõü=[0.8, 0.5])
#+END_src

:RESULTS:
: L(œï‚ÇÄ=0.8, œï‚ÇÅ=0.5) = 0.22852499999999987
[[file:./.ob-jupyter/eecc6c943abebcc3ca18f795284140a3acd5ba49.svg]]
:END:

#+begin_src jupyter-julia :session jl :async yes
heatmap(0.75:0.01:0.9, 0.45:0.01:0.6, (œï‚ÇÄ, œï‚ÇÅ) -> L([œï‚ÇÄ, œï‚ÇÅ]),
    clims=(0.2, 0.25),
    c=cgrad(:viridis, rev=true, scale=:exp),
    xlabel=L"\phi_0",
    ylabel=L"\phi_1"
)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/19af3c14f7f3f62fa4a91aa4c55cba3429f4148b.svg]]
* Problems 2.1--2.3 (Supervised Learning)

*2.1.* /To walk ‚Äúdownhill‚Äù on the loss function (equation 2.5), we measure its gradient with respect to the parameters \( \phi_0 \) and \( \phi_1 \). Calculate expressions for the slopes \( \partial L / \partial \phi_0 \) and \( \partial L / \partial \phi_1 \)./

\begin{align*}
\frac{\partial L}{\partial \phi_0} &= \frac{\partial}{\partial \phi_0} \sum_{i=1}^I (\mathrm{f}[x_i, \boldsymbol{\phi}] - y_i)^2 \\
&= \sum_{i=1}^I \frac{\partial}{\partial \phi_0} (\phi_0 + \phi_1 x_i - y_i)^2 \\
&= \sum 2(\phi_0 + \phi_1 x_i - y_i) \cdot \frac{\partial}{\partial{\phi_0}} (\phi_0 + \phi_1 x_i - y_i) \\
&= 2 \sum_{i=1}^I (\phi_0 + \phi_1 x_i - y_i)
\end{align*}

\begin{align*}
\frac{\partial L}{\partial \phi_1} &= \frac{\partial}{\partial \phi_1} \sum_{i=1}^I (\mathrm{f}[x_i, \boldsymbol{\phi}] - y_i)^2 \\
&= \sum_{i=1}^I \frac{\partial}{\partial \phi_1} (\phi_0 + \phi_1 x_i - y_i)^2 \\
&= \sum 2 (\phi_0 + \phi_1 x_i - y_i) \cdot \frac{\partial}{\partial \phi_1} (\phi_0 + \phi_1 x_i - y_i) \\
&= \sum 2 (\phi_0 + \phi_1 x_i - y_i) \cdot x_i \\
&= 2 \sum_{i=0}^I x_i (\phi_0 + \phi_1 x_i - y_i)
\end{align*}

*2.2.* /Show that we can find the minimum of the loss function in closed form by setting the expression for the derivatives from problem 2.1 to zero and solving for \( \phi_0 \) and \( \phi_1 \). Note that this works for linear regression but not for more complex models; this is why we use iterative model fitting methods like gradient descent (figure 2.4)./

\begin{align*}
\frac{\partial L}{\partial \phi_0} = {} & 2 \sum_{i=1}^I (\phi_0 + \phi_1 x_i - y_i) = 0 \\
\implies & I \phi_0 + \phi_1 \sum x_i - \sum y_i = 0 \\
\implies & \phi_0 = \frac{\sum y_i}{I} - \phi_1 \frac{\sum x_i}{I} \\
\implies & \phi_0 = \bar{y} - \phi_1 \bar{x}
\end{align*}

\begin{align*}
\frac{\partial L}{\partial \phi_1} = {} & 2 \sum_{i=1}^I x_i (\phi_0 + \phi_1 x_i - y_i) = 0 \\
\implies & \sum (\phi_1 x_i^2 + \phi_0 x_i - x_i y_i) = 0 \\
\implies & \phi_1 \sum  x_i^2 = \sum x_i y_i - \phi_0 \sum x_i \\
\implies & \phi_1 \sum  x_i^2 = \sum x_i y_i - (\bar{y} - \phi_1 \bar{x}) \sum x_i \\
\implies & \phi_1 \sum  x_i^2 = \sum x_i y_i - \bar{y} \sum x_i + \phi_1 \bar{x} \sum x_i \\
\implies & \phi_1 \left( \sum x_i^2 - \bar{x} \sum x_i \right) = \sum x_i y_i - \bar{y} \sum x_i \\
\implies & \phi_1 = \frac{\sum x_i y_i - \bar{y} \sum x_i}{\sum x_i^2 - \bar{x} \sum x_i} \\
\implies & \phi_1 = \frac{\sum x_i y_i - \bar{y} \sum x_i}{\sum x_i^2 - I \bar{x}^2} \\
\implies & \phi_1 = \frac{\sum x_i y_i - I \bar{x} \bar{y}}{\sum (x_i - \bar{x})^2} \\
\implies & \phi_1 = \frac{\sum_{i=1}^I (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^I (x_i - \bar{x})^2}
\end{align*}

\begin{align*}
\frac{\partial L}{\partial \phi_{1}} = {} & 2 \sum_{i=1}^I x_i (\phi_{0} + \phi_{1} x_i - y_i) = 0 \\
\implies & \sum (\phi_{1} x_i^{2} + \phi_{0} x_i - x_i y_i) = 0 \\
\implies & \phi_{1} \sum x_i^{2} = \sum x_i y_i - \phi_{0} \sum x_i \\
\implies & \phi_{1} \sum x_i^{2} = \sum x_i y_i - (\bar{y} - \phi_{1} \bar{x}) \sum x_i \\
\implies & \phi_{1} \sum x_i^{2} = \sum x_i y_i - \bar{y} \sum x_i + \phi_{1} \bar{x} \sum x_i \\
\implies & \phi_{1} \left( \sum x_i^{2} - \bar{x} \sum x_i \right) = \sum x_i y_i - \bar{y} \sum x_i \\
\implies & \phi_{1} = \frac{\sum x_i y_i - \bar{y} \sum x_i}{\sum x_i^{2} - \bar{x} \sum x_i} \\
\implies & \phi_{1} = \frac{\sum x_i y_i - \bar{y} \sum x_i}{\sum x_i^{2} - I \bar{x}^{2}} \\
\implies & \phi_{1} = \frac{\sum x_i y_i - I \bar{x} \bar{y}}{\sum (x_i - \bar{x})^{2}} \\
\implies & \phi_{1} = \frac{\sum_{i=1}^I (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^I (x_i - \bar{x})^{2}}
\end{align*}

\begin{align*}
\sum (x_i - \bar x)^2 &= \sum (x_i^2 - 2 x_i \bar x + \bar x^2) \\
&= \sum x_i^2 - 2 \bar x \sum x_i + I \bar x^2 \\
&= \sum x_i^2 - 2 \bar x(I\bar x) + I \bar x^2 \\
&= \sum x_i^2 - 2 I \bar x^2 + I \bar x^2 \\
&= \sum x_i^2 - I \bar x^2
\end{align*} 

\begin{align*}
\sum (x_i - \bar x) (y_i - \bar y) &= \sum (x_i y_i - x_i \bar y - \bar x y_i + \bar x \bar y) \\
&= \sum x_i y_i - \bar y \sum x_i - \bar x \sum y_i + I \bar x \bar y \\
&= \sum x_i y_i - \bar y (I \bar x) - \bar x (I \bar y) + I \bar x \bar y \\
&= \sum x_i y_i - I \bar x \bar y
\end{align*}

*2.3.* /Consider reformulating linear regression as a generative model, so we have \( x = \mathrm{g}[y, \boldsymbol{\phi}] = \phi_{0} + \phi_{1} y \). What is the new loss function? Find an expression for the inverse function \( y = \mathrm{g}^{-1}[x, \boldsymbol{\phi}] \) that we would use to perform inference. Will this model make the same predictions as the discriminative version for a given training dataset \( \{x_i, y_i\} \)? One way to establish this is to write code that ftis a line to three data points using both methods and see if the result is the same./
